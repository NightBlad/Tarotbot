# Copy this file to .env and fill in the values

# ==================== DISCORD BOT CONFIGURATION ====================

# Your Discord bot token
DISCORD_TOKEN=your_discord_bot_token_here

# If you enable this to 'true' (and enable Message Content in the Discord
# Developer Portal for your bot) the bot will be able to read message text
# and respond to prefix commands like `!tarot one`. Defaults to false.
# Note: enabling this requires you to enable the privileged Message Content
# intent in the Developer Portal and comply with Discord policies.
DISCORD_MESSAGE_CONTENT=false

# ==================== SERVER CONFIGURATION ====================

# Web server port
PORT=8080
WEB_PORT=8080

# Node environment (development/production)
NODE_ENV=development

# CORS origin (set to your domain in production)
CORS_ORIGIN=*

# Session secret (CHANGE THIS IN PRODUCTION!)
SESSION_SECRET=tarot-mystic-secret-key-change-in-production

# ==================== EXTERNAL API CONFIGURATION ====================

# Base URL of the Tarot API server (include protocol and port if needed)
# e.g. http://localhost:3000
TAROT_API_URL=https://tarotbot-astc.onrender.com

# LangFlow API Configuration
LANGFLOW_API_URL=https://your-langflow-instance.com/api/v1/run/{flow}
LANGFLOW_API_KEY=your-api-key-here
LANGFLOW_AUTH_HEADER=Authorization

# ==================== PERFORMANCE TUNING ====================

# Rate Limiting (per IP address)
RATE_LIMIT_WINDOW=60000          # 1 minute in milliseconds
RATE_LIMIT_MAX=30                # Max requests per window

# LangFlow Rate Limiting (per IP address)
LANGFLOW_RATE_WINDOW=60000       # 1 minute in milliseconds
LANGFLOW_RATE_MAX=10             # Max LangFlow calls per window

# LangFlow Queue Configuration
LANGFLOW_CONCURRENCY=3           # Max concurrent LangFlow API calls

# Response Cache Configuration
CACHE_MAX_ITEMS=500              # Max number of cached readings
CACHE_TTL=3600000                # Cache time-to-live (1 hour in milliseconds)

# ==================== MULTI-USER OPTIMIZATION NOTES ====================
#
# RATE_LIMIT_MAX: Controls how many total API requests a single IP can make
#   - Default: 30 requests/minute
#   - Increase for high-traffic sites, decrease to prevent abuse
#
# LANGFLOW_RATE_MAX: Limits reading requests to prevent AI API overload
#   - Default: 10 readings/minute per IP
#   - Adjust based on your LangFlow API quota
#
# LANGFLOW_CONCURRENCY: Max simultaneous LangFlow calls across ALL users
#   - Default: 3 concurrent requests
#   - Higher = faster for users, but may overwhelm LangFlow API
#   - Lower = better queue management, slower response
#
# CACHE_MAX_ITEMS: Number of readings to keep in memory cache
#   - Default: 500 readings
#   - Increase if you have RAM and want higher cache hit rate
#   - Each cached reading saves ~1 LangFlow API call
#
# CACHE_TTL: How long to keep readings in cache
#   - Default: 1 hour (3600000ms)
#   - Increase for more cache hits (but older results)
#   - Decrease for fresher results (but more API calls)
#
# RECOMMENDED SETTINGS FOR PRODUCTION:
#   - Small site (< 100 users/day): Default settings work well
#   - Medium site (100-1000 users/day): 
#       LANGFLOW_CONCURRENCY=5
#       CACHE_MAX_ITEMS=1000
#       CACHE_TTL=7200000 (2 hours)
#   - Large site (> 1000 users/day):
#       LANGFLOW_CONCURRENCY=10
#       CACHE_MAX_ITEMS=2000
#       RATE_LIMIT_MAX=50
#       Consider using Redis for session/cache storage
#
# MONITORING:
#   - Visit /api/status to see real-time metrics
#   - Watch cache hit rate - aim for 30%+ for efficiency
#   - Monitor queue length - if consistently high, increase LANGFLOW_CONCURRENCY

